"""
Utilidades para an√°lisis exploratorio de datos (EDA)
Funciones para an√°lisis autom√°tico de estructura de datasets
"""

import pandas as pd
import numpy as np
from collections import Counter


def analizar_estructura_dataset(df, nombre_dataset="Dataset", mostrar_jerarquia=True, 
                               target_candidates=['PAPA', 'PROME_ACADE', 'AVANCE_CARRERA', 
                                                'NUMERO_MATRICULAS', 'PUNTAJE_ADMISION']):
    """
    Realiza un an√°lisis completo de la estructura de un dataset
    
    Parameters:
    -----------
    df : pandas.DataFrame
        Dataset a analizar
    nombre_dataset : str
        Nombre descriptivo del dataset para mostrar en el reporte
    mostrar_jerarquia : bool
        Si mostrar el an√°lisis de estructura jer√°rquica
    target_candidates : list
        Lista de variables candidatas a ser target
        
    Returns:
    --------
    dict : Diccionario con los resultados del an√°lisis
    """
    
    print("=" * 80)
    print(f"üìä AN√ÅLISIS DE ESTRUCTURA DEL {nombre_dataset.upper()}")
    print("=" * 80)

    # 1. DIMENSIONALIDAD
    print("\nüî¢ 1. DIMENSIONALIDAD")
    print("-" * 30)
    print(f"N√∫mero de filas (registros): {df.shape[0]:,}")
    print(f"N√∫mero de columnas (variables): {df.shape[1]:,}")
    print(f"Total de celdas: {df.shape[0] * df.shape[1]:,}")

    # 2. INFORMACI√ìN GENERAL
    print(f"\nTama√±o en memoria: {df.memory_usage(deep=True).sum() / 1024:.2f} KB")

    # 3. AN√ÅLISIS DE TIPOS DE VARIABLES
    print("\nüìù 2. AN√ÅLISIS DE TIPOS DE VARIABLES")
    print("-" * 40)

    # Identificar tipos de variables autom√°ticamente
    numeric_vars = []
    categorical_vars = []
    binary_vars = []
    date_vars = []
    id_vars = []

    for col in df.columns:
        # Verificar si es fecha
        if 'FECHA' in col.upper():
            date_vars.append(col)
        # Verificar si es ID o c√≥digo
        elif any(keyword in col.upper() for keyword in ['COD_', 'CODIGO', 'DOCUMENTO']):
            id_vars.append(col)
        else:
            # Analizar el contenido de la columna
            non_null_values = df[col].dropna()
            unique_values = non_null_values.unique()
            
            # Variables binarias (solo 2 valores √∫nicos)
            if len(unique_values) == 2:
                binary_vars.append(col)
            # Variables num√©ricas
            elif df[col].dtype in ['int64', 'float64']:
                numeric_vars.append(col)
            # Variables categ√≥ricas
            else:
                categorical_vars.append(col)

    print(f"üî¢ Variables Num√©ricas ({len(numeric_vars)}):")
    for var in numeric_vars:
        print(f"   ‚Ä¢ {var}")

    print(f"\nüìÇ Variables Categ√≥ricas ({len(categorical_vars)}):")
    for var in categorical_vars:
        unique_count = df[var].nunique()
        print(f"   ‚Ä¢ {var} ({unique_count} categor√≠as √∫nicas)")

    print(f"\n‚ö° Variables Binarias ({len(binary_vars)}):")
    for var in binary_vars:
        values = df[var].dropna().unique()
        print(f"   ‚Ä¢ {var} (valores: {', '.join(map(str, values))})")

    print(f"\nüìÖ Variables de Fecha ({len(date_vars)}):")
    for var in date_vars:
        print(f"   ‚Ä¢ {var}")

    print(f"\nüîë Variables Identificadoras/C√≥digos ({len(id_vars)}):")
    for var in id_vars:
        unique_count = df[var].nunique()
        print(f"   ‚Ä¢ {var} ({unique_count} valores √∫nicos)")

    # 4. AN√ÅLISIS DE CLAVES √öNICAS E IDENTIFICADORES
    print("\nüîë 3. AN√ÅLISIS DE CLAVES √öNICAS E IDENTIFICADORES")
    print("-" * 50)

    # Buscar posibles claves primarias
    primary_key_candidates = []
    for col in df.columns:
        if df[col].nunique() == len(df) and not df[col].isnull().any():
            primary_key_candidates.append(col)

    print("üéØ Candidatos a Clave Primaria:")
    if primary_key_candidates:
        for candidate in primary_key_candidates:
            print(f"   ‚Ä¢ {candidate}")
    else:
        print("   ‚Ä¢ No se encontraron claves primarias √∫nicas")

    # An√°lisis de duplicados
    duplicated_rows = df.duplicated().sum()
    print(f"\nüìã An√°lisis de Duplicados:")
    print(f"   ‚Ä¢ Filas duplicadas completas: {duplicated_rows}")

    # Buscar posibles claves for√°neas (c√≥digos que se repiten)
    print("\nüîó Posibles Claves For√°neas (c√≥digos con m√∫ltiples referencias):")
    foreign_key_candidates = []
    for col in id_vars:
        unique_count = df[col].nunique()
        total_count = len(df[col].dropna())
        if unique_count < total_count and unique_count > 1:
            foreign_key_candidates.append((col, unique_count, total_count))

    for fk, unique, total in sorted(foreign_key_candidates, key=lambda x: x[1]):
        repetition_rate = (total - unique) / total * 100
        print(f"   ‚Ä¢ {fk}: {unique} valores √∫nicos, {total} registros ({repetition_rate:.1f}% repetici√≥n)")

    # 5. AN√ÅLISIS DE VALORES FALTANTES
    print("\n‚ùì 4. AN√ÅLISIS DE VALORES FALTANTES")
    print("-" * 40)

    missing_analysis = []
    for col in df.columns:
        missing_count = df[col].isnull().sum()
        missing_pct = (missing_count / len(df)) * 100
        if missing_count > 0:
            missing_analysis.append((col, missing_count, missing_pct))

    if missing_analysis:
        missing_analysis.sort(key=lambda x: x[2], reverse=True)
        print("Variables con valores faltantes:")
        for col, count, pct in missing_analysis:
            print(f"   ‚Ä¢ {col}: {count} valores faltantes ({pct:.1f}%)")
    else:
        print("‚úÖ No se encontraron valores faltantes")

    # 6. ESTAD√çSTICAS DESCRIPTIVAS B√ÅSICAS
    print("\nüìà 5. ESTAD√çSTICAS DESCRIPTIVAS DE VARIABLES NUM√âRICAS")
    print("-" * 55)

    if numeric_vars:
        numeric_stats = df[numeric_vars].describe()
        print(numeric_stats.round(2))
    else:
        print("No hay variables num√©ricas para analizar")

    # 7. RESUMEN DE CARDINALIDAD
    print("\nüéØ 6. RESUMEN DE CARDINALIDAD (Top variables con m√°s categor√≠as)")
    print("-" * 65)

    cardinality_analysis = []
    for col in df.columns:
        unique_count = df[col].nunique()
        cardinality_analysis.append((col, unique_count))

    cardinality_analysis.sort(key=lambda x: x[1], reverse=True)

    print("Top 10 variables por n√∫mero de valores √∫nicos:")
    for i, (col, unique_count) in enumerate(cardinality_analysis[:10], 1):
        percentage = (unique_count / len(df)) * 100
        print(f"{i:2d}. {col}: {unique_count} valores √∫nicos ({percentage:.1f}% del total)")

    # 8. AN√ÅLISIS ESPEC√çFICO PARA MACHINE LEARNING
    print("\nü§ñ 7. CONSIDERACIONES PARA MACHINE LEARNING")
    print("-" * 45)

    print("üìä Variables Target Potenciales:")
    available_targets = [var for var in target_candidates if var in df.columns]

    for var in available_targets:
        if df[var].dtype in ['int64', 'float64']:
            non_null_count = df[var].count()
            min_val = df[var].min()
            max_val = df[var].max()
            print(f"   ‚Ä¢ {var}: {non_null_count} valores v√°lidos, rango [{min_val} - {max_val}]")

    print(f"\nüîÑ Variables que requieren preprocesamiento:")
    preprocessing_needed = []

    # Variables categ√≥ricas con alta cardinalidad
    high_cardinality = [(col, df[col].nunique()) for col in categorical_vars if df[col].nunique() > 20]
    if high_cardinality:
        print("   üìÇ Categ√≥ricas con alta cardinalidad (>20 categor√≠as):")
        for col, count in high_cardinality:
            print(f"      ‚Ä¢ {col}: {count} categor√≠as")
            preprocessing_needed.append(f"{col} (encoding)")

    # Variables de fecha
    if date_vars:
        print("   üìÖ Variables de fecha (requieren feature engineering):")
        for var in date_vars:
            print(f"      ‚Ä¢ {var}")
            preprocessing_needed.append(f"{var} (datetime features)")

    # Variables con valores faltantes significativos
    high_missing = [(col, pct) for col, count, pct in missing_analysis if pct > 10]
    if high_missing:
        print("   ‚ùì Variables con >10% valores faltantes:")
        for col, pct in high_missing:
            print(f"      ‚Ä¢ {col}: {pct:.1f}% faltantes")
            preprocessing_needed.append(f"{col} (imputation)")

    print(f"\n‚öôÔ∏è Recomendaciones de preprocesamiento:")
    if preprocessing_needed:
        for i, rec in enumerate(preprocessing_needed, 1):
            print(f"   {i}. {rec}")
    else:
        print("   ‚úÖ Datos parecen estar en buen estado para ML")

    # 9. ESTRUCTURA DE DATOS JER√ÅRQUICA (opcional)
    if mostrar_jerarquia:
        print("\nüèóÔ∏è 8. ESTRUCTURA JER√ÅRQUICA IDENTIFICADA")
        print("-" * 45)

        hierarchy_levels = [
            ('SEDE', 'Nivel Universidad'),
            ('COD_FACULTAD', 'FACULTAD', 'Nivel Facultad'),
            ('COD_PLAN', 'PLAN', 'Nivel Programa'),
            ('DOCUMENTO', 'Nivel Estudiante')
        ]

        print("Jerarqu√≠a de datos identificada:")
        for level in hierarchy_levels:
            if len(level) == 3:
                code_col, name_col, description = level
                if code_col in df.columns and name_col in df.columns:
                    unique_codes = df[code_col].nunique()
                    unique_names = df[name_col].nunique()
                    print(f"   ‚Ä¢ {description}: {unique_codes} c√≥digos, {unique_names} nombres")
            else:
                col, description = level
                if col in df.columns:
                    unique_count = df[col].nunique()
                    print(f"   ‚Ä¢ {description}: {unique_count} registros √∫nicos")

    print("\n" + "=" * 80)
    print("‚úÖ AN√ÅLISIS COMPLETADO")
    print("=" * 80)

    # Mostrar un resumen final
    print(f"\nüìã RESUMEN EJECUTIVO:")
    print(f"   ‚Ä¢ Dataset con {df.shape[0]:,} registros y {df.shape[1]} variables")
    print(f"   ‚Ä¢ {len(numeric_vars)} variables num√©ricas, {len(categorical_vars)} categ√≥ricas")
    print(f"   ‚Ä¢ {len(binary_vars)} variables binarias, {len(date_vars)} de fecha")
    print(f"   ‚Ä¢ {len(id_vars)} identificadores/c√≥digos")
    print(f"   ‚Ä¢ {len([x for x in missing_analysis if x[2] > 0])} variables con datos faltantes")
    print(f"   ‚Ä¢ Listo para aplicar ML con preprocesamiento adecuado")
    
    # Retornar resultados estructurados
    resultados = {
        'dimensiones': df.shape,
        'tipos_variables': {
            'numericas': numeric_vars,
            'categoricas': categorical_vars,
            'binarias': binary_vars,
            'fechas': date_vars,
            'identificadores': id_vars
        },
        'claves_primarias': primary_key_candidates,
        'duplicados': duplicated_rows,
        'valores_faltantes': missing_analysis,
        'cardinalidad': cardinality_analysis,
        'targets_disponibles': available_targets,
        'preprocesamiento_requerido': preprocessing_needed
    }
    
    return resultados


def analisis_rapido_dataset(df, nombre_dataset="Dataset"):
    """
    Versi√≥n simplificada del an√°lisis para obtener solo informaci√≥n b√°sica
    
    Parameters:
    -----------
    df : pandas.DataFrame
        Dataset a analizar
    nombre_dataset : str
        Nombre descriptivo del dataset
        
    Returns:
    --------
    dict : Resumen b√°sico del dataset
    """
    print(f"üìä RESUMEN R√ÅPIDO: {nombre_dataset}")
    print("-" * 40)
    
    # Informaci√≥n b√°sica
    print(f"Dimensiones: {df.shape[0]:,} filas √ó {df.shape[1]} columnas")
    print(f"Memoria: {df.memory_usage(deep=True).sum() / 1024:.2f} KB")
    
    # Tipos de datos
    tipos = df.dtypes.value_counts()
    print(f"Tipos de datos: {dict(tipos)}")
    
    # Valores faltantes
    missing = df.isnull().sum().sum()
    missing_pct = (missing / (df.shape[0] * df.shape[1])) * 100
    print(f"Valores faltantes: {missing} ({missing_pct:.2f}%)")
    
    # Duplicados
    duplicados = df.duplicated().sum()
    print(f"Filas duplicadas: {duplicados}")
    
    return {
        'shape': df.shape,
        'memory_kb': df.memory_usage(deep=True).sum() / 1024,
        'dtypes': dict(tipos),
        'missing_values': missing,
        'missing_percent': missing_pct,
        'duplicated_rows': duplicados
    }


def comparar_datasets(df_antes, df_despues, nombre_antes="Dataset Original", nombre_despues="Dataset Procesado"):
    """
    Compara dos datasets y muestra las diferencias principales
    
    Parameters:
    -----------
    df_antes : pandas.DataFrame
        Dataset antes del procesamiento
    df_despues : pandas.DataFrame
        Dataset despu√©s del procesamiento
    nombre_antes : str
        Nombre del primer dataset
    nombre_despues : str
        Nombre del segundo dataset
    """
    print("üîÑ COMPARACI√ìN DE DATASETS")
    print("=" * 50)
    
    # Comparar dimensiones
    print(f"\nüìè DIMENSIONES:")
    print(f"   {nombre_antes}: {df_antes.shape[0]:,} √ó {df_antes.shape[1]}")
    print(f"   {nombre_despues}: {df_despues.shape[0]:,} √ó {df_despues.shape[1]}")
    
    filas_diff = df_despues.shape[0] - df_antes.shape[0]
    cols_diff = df_despues.shape[1] - df_antes.shape[1]
    print(f"   Diferencia: {filas_diff:+} filas, {cols_diff:+} columnas")
    
    # Comparar valores faltantes
    missing_antes = df_antes.isnull().sum().sum()
    missing_despues = df_despues.isnull().sum().sum()
    print(f"\n‚ùì VALORES FALTANTES:")
    print(f"   {nombre_antes}: {missing_antes}")
    print(f"   {nombre_despues}: {missing_despues}")
    print(f"   Reducci√≥n: {missing_antes - missing_despues} valores faltantes")
    
    # Comparar columnas
    cols_antes = set(df_antes.columns)
    cols_despues = set(df_despues.columns)
    cols_nuevas = cols_despues - cols_antes
    cols_eliminadas = cols_antes - cols_despues
    
    print(f"\nüìÇ COLUMNAS:")
    if cols_nuevas:
        print(f"   Nuevas columnas ({len(cols_nuevas)}): {', '.join(list(cols_nuevas)[:5])}{'...' if len(cols_nuevas) > 5 else ''}")
    if cols_eliminadas:
        print(f"   Columnas eliminadas ({len(cols_eliminadas)}): {', '.join(list(cols_eliminadas)[:5])}{'...' if len(cols_eliminadas) > 5 else ''}")
    
    print(f"   Columnas comunes: {len(cols_antes & cols_despues)}")
